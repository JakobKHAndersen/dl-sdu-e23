{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Deep learning frameworks\n",
        "\n",
        "Deep learning frameworks allow for creating complex neural networks that runs on optimized hardware such as GPUs and TPUs using higher level programming languages like python with APIs to a lower level platform. In this notebook, examples of building and training a convolutional neural network are provided. Examples are given in tensorflow/keras, PyTorch and JAX. Each framework provides a more a less simple API to define neural networks, with different levels of abstraction and degress of allowed customization.\n",
        "\n",
        "Currently, Tensoflow/keras and Pytorch are the two most used frameworks used in research and industry, with Pytorch gaining and edge in the research community. JAX is little less mature and you probably shoudtn't use it in this course when you start to develop and train networks for assignment 2 and 3. It is a research project from google which aims to provide even more customizability with acces to control of more low level features to improve efficiency of neural network training on specialized hardware.    "
      ],
      "metadata": {
        "id": "f0Gt2MiYZkuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keras\n",
        "\n",
        "(from https://keras.io/about/):\n",
        "\n",
        "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
        "\n",
        "Keras is:\n",
        "\n",
        "    Simple -- but not simplistic. Keras reduces developer cognitive load to free you to focus on the parts of the problem that really matter.\n",
        "    Flexible -- Keras adopts the principle of progressive disclosure of complexity: simple workflows should be quick and easy, while arbitrarily advanced workflows should be possible via a clear path that builds upon what you've already learned.\n",
        "    Powerful -- Keras provides industry-strength performance and scalability: it is used by organizations and companies including NASA, YouTube, or Waymo.\n",
        "\n",
        "\n",
        "TensorFlow 2 is an end-to-end, open-source machine learning platform. You can think of it as an infrastructure layer for differentiable programming. It combines four key abilities:\n",
        "\n",
        "    Efficiently executing low-level tensor operations on CPU, GPU, or TPU.\n",
        "    Computing the gradient of arbitrary differentiable expressions.\n",
        "    Scaling computation to many devices, such as clusters of hundreds of GPUs.\n",
        "    Exporting programs (\"graphs\") to external runtimes such as servers, browsers, mobile and embedded devices.\n",
        "\n",
        "Keras is the high-level API of the TensorFlow platform: an approachable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity.\n",
        "\n",
        "Keras empowers engineers and researchers to take full advantage of the scalability and cross-platform capabilities of the TensorFlow platform: you can run Keras on TPU or on large clusters of GPUs, and you can export your Keras models to run in the browser or on a mobile device.\n",
        "\n",
        "The core data structures of Keras are layers and models. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers, or write models entirely from scratch via subclasssing."
      ],
      "metadata": {
        "id": "fZ_GYX40ZvFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Sequential API\n",
        "\n",
        "here's an example of a VGG-style Convolutional Neural Network (CNN) implemented using the Sequential API in Keras"
      ],
      "metadata": {
        "id": "9lk4xBcuaBuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the VGG-style CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Block 1\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 2\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 3\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 4\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# Block 5\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "# Flatten the output for fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dense(1000, activation='softmax'))  # 1000 is the number of output classes (adjust as needed)\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hKtBACa1aOKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a VGG-style CNN with five convolutional blocks and three fully connected layers.\n",
        "You can adjust the input shape and the number of output classes as needed for your specific task.\n",
        "Additionally, you can compile the model and train it on your dataset using appropriate data preprocessing and training code."
      ],
      "metadata": {
        "id": "6AMGm4yLawaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Functional API\n",
        "\n",
        "Here's the same VGG-style Convolutional Neural Network (CNN) model implemented\n",
        "using the Functional API in Keras:"
      ],
      "metadata": {
        "id": "IP34zvpiaqJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Block 1\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "# Block 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "# Block 3\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "# Block 4\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "# Block 5\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "# Flatten the output for fully connected layers\n",
        "x = Flatten()(x)\n",
        "\n",
        "# Fully connected layers\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "output_layer = Dense(1000, activation='softmax')(x)  # 1000 is the number of output classes (adjust as needed)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "v2zaI6uKa9HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the same VGG-style CNN model using the Functional API in Keras.\n",
        "As with the previous example, you can adjust the input shape and the number of output classes as needed for your specific task.\n",
        "To use this model, you can compile it and train it on your dataset using appropriate data preprocessing and training code."
      ],
      "metadata": {
        "id": "FK-g54C2bFAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Subclassing\n",
        "\n",
        "you can also implement the VGG-style CNN using subclassing in Keras. Here's how you can do it:"
      ],
      "metadata": {
        "id": "TF_eZ9NFbSXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "class VGGModel(tf.keras.Model):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(VGGModel, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        self.conv1_1 = Conv2D(64, (3, 3), activation='relu', padding='same')\n",
        "        self.conv1_2 = Conv2D(64, (3, 3), activation='relu', padding='same')\n",
        "        self.maxpool1 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "\n",
        "        # Block 2\n",
        "        self.conv2_1 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
        "        self.conv2_2 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
        "        self.maxpool2 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "\n",
        "        # Block 3\n",
        "        self.conv3_1 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv3_2 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv3_3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.maxpool3 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "\n",
        "        # Block 4\n",
        "        self.conv4_1 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.conv4_2 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.conv4_3 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.maxpool4 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "\n",
        "        # Block 5\n",
        "        self.conv5_1 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.conv5_2 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.conv5_3 = Conv2D(512, (3, 3), activation='relu', padding='same')\n",
        "        self.maxpool5 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.flatten = Flatten()\n",
        "        self.fc1 = Dense(4096, activation='relu')\n",
        "        self.fc2 = Dense(4096, activation='relu')\n",
        "        self.fc3 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1_1(inputs)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.conv3_3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.conv4_3(x)\n",
        "        x = self.maxpool4(x)\n",
        "\n",
        "        x = self.conv5_1(x)\n",
        "        x = self.conv5_2(x)\n",
        "        x = self.conv5_3(x)\n",
        "        x = self.maxpool5(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the VGGModel class\n",
        "model = VGGModel(num_classes=1000)\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.build((None, 224, 224, 3))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mTvsKaNwbfIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this code, we create a custom subclass of tf.keras.Model called VGGModel and define the layers and operations in the __init__ method.\n",
        "The call method specifies the forward pass of the model. You can adjust the number of output classes by passing the num_classes parameter\n",
        "when creating an instance of the model. Finally, we build the model and print a summary of its architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "BksBQjspbuGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functional v. Sequential\n",
        "\n",
        "The functional API makes it possible to build more complex architectures where layers are not neccesarrily arranged sequentialy one after another, but instead the network branches out, and reconnects in different parts, or networks with mulitple inputs/outputs.\n"
      ],
      "metadata": {
        "id": "TfevBjeOjTNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resnet example\n",
        "\n",
        "example of defining a ResNet-style model using the Keras Functional API. The ResNet architecture is known for its deep structure with residual blocks. In this example, We will define a simplified ResNet with just a few residual blocks for demonstration purposes:"
      ],
      "metadata": {
        "id": "I9rppb5Rj7in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def residual_block(x, filters, stride=1):\n",
        "    shortcut = x\n",
        "\n",
        "    # First convolution layer\n",
        "    x = Conv2D(filters, (3, 3), strides=stride, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    # Second convolution layer\n",
        "    x = Conv2D(filters, (3, 3), strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Shortcut connection\n",
        "    if stride != 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, (1, 1), strides=stride, padding='same')(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Define input layer\n",
        "input_shape = (224, 224, 3)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "# Initial convolution layer\n",
        "x = Conv2D(64, (7, 7), strides=2, padding='same')(input_tensor)\n",
        "x = BatchNormalization()(x)\n",
        "x = ReLU()(x)\n",
        "\n",
        "# Residual blocks\n",
        "num_blocks = 3  # Number of residual blocks\n",
        "num_filters = 64  # Number of filters in each block\n",
        "\n",
        "for _ in range(num_blocks):\n",
        "    x = residual_block(x, num_filters)\n",
        "\n",
        "# Global Average Pooling\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Fully connected layer for classification\n",
        "num_classes = 10  # Number of output classes\n",
        "output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the ResNet model\n",
        "resnet_model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "\n",
        "# Print model summary\n",
        "resnet_model.summary()"
      ],
      "metadata": {
        "id": "9DrBejggj461"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example:\n",
        "\n",
        "    We define a residual_block function that represents a single residual block in the ResNet architecture. It consists of two convolutional layers with batch normalization and a shortcut connection.\n",
        "\n",
        "    We use this residual_block function to stack multiple residual blocks together.\n",
        "\n",
        "    The model starts with an initial convolutional layer, followed by the stack of residual blocks.\n",
        "\n",
        "    After the stack of residual blocks, we apply global average pooling to reduce the spatial dimensions.\n",
        "\n",
        "    Finally, we add a fully connected layer for classification with the desired number of output classes.\n"
      ],
      "metadata": {
        "id": "FemlquBSkSQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Siamese network\n",
        "\n",
        "The Keras Functional API allows for more flexible model architectures compared to the Sequential API because it supports multiple inputs and outputs and enables you to create complex network structures. Here's an example of a model that cannot be easily written using the Sequential API: a Siamese Network for image similarity comparison.\n",
        "\n",
        "A Siamese Network is used for tasks like face recognition or similarity-based retrieval. It learns to differentiate between pairs of input samples, often used in scenarios where you want to determine the similarity or dissimilarity between two inputs. Here's how you can define a Siamese Network using the Keras Functional API:"
      ],
      "metadata": {
        "id": "e4iTphfdkgNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, MaxPooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# Define the base network (shared weights)\n",
        "input_shape = (28, 28, 1)\n",
        "input_left = Input(shape=input_shape)\n",
        "input_right = Input(shape=input_shape)\n",
        "\n",
        "shared_conv = Sequential([\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten()\n",
        "])\n",
        "\n",
        "output_left = shared_conv(input_left)\n",
        "output_right = shared_conv(input_right)\n",
        "\n",
        "# Define a custom layer to compute the L1 distance\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "distance_layer = Lambda(euclidean_distance, output_shape=lambda x: x[0])\n",
        "\n",
        "# Connect the inputs and compute the distance\n",
        "distance = distance_layer([output_left, output_right])\n",
        "\n",
        "# Define the final model\n",
        "siamese_model = Model(inputs=[input_left, input_right], outputs=distance)\n",
        "\n",
        "# Print the model summary\n",
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "-DqNyAcVktVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Siamese Network:\n",
        "\n",
        "    We define two separate input layers, input_left and input_right, which represent two images to be compared.\n",
        "\n",
        "    We define a shared convolutional base, shared_conv, that processes both input images. This base is shared between the two branches of the network.\n",
        "\n",
        "    We define a custom layer, euclidean_distance, which calculates the Euclidean distance between the output vectors of the shared base for the two input images.\n",
        "\n",
        "    We use the Lambda layer to apply the custom distance function to the output vectors of the shared base.\n",
        "\n",
        "    The final model, siamese_model, takes two input images and outputs the Euclidean distance between their representations.\n",
        "\n",
        "This Siamese Network architecture is an example of a model that requires the flexibility of the Functional API due to its multiple inputs and custom layer for computing the distance between the inputs. It's commonly used for various similarity-based tasks, including image similarity and face recognition."
      ],
      "metadata": {
        "id": "o-eUMG6xmD7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pytorch\n",
        "\n",
        "PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library. Torch is an open source ML library used for creating deep neural networks and is written in the Lua scripting language. It's one of the preferred platforms for deep learning research. The framework is built to speed up the process between research prototyping and deployment. Like Keras, Pytorch has different API such as functional and sequential which can be used to construct various kinds of Neural networks"
      ],
      "metadata": {
        "id": "Ok7tEwNSb1DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequential\n",
        "you can implement a VGG-style CNN in PyTorch as well.\n",
        "Here's an example of how you can do it:"
      ],
      "metadata": {
        "id": "Rm30ea6LcIiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the VGG model\n",
        "model = VGG(num_classes=1000)\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "ktqIlO_WcL5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a VGG model in PyTorch, similar to the Keras implementation.\n",
        "You can adjust the number of output classes by passing the num_classes parameter when creating an instance of the model.\n",
        "To use this model, you can load your data, define a loss function, and perform training as needed in PyTorch."
      ],
      "metadata": {
        "id": "mRKXBqaccqc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functional\n",
        "\n",
        "PyTorch provides a functional API that allows you to build neural networks in a manner similar to the functional API in Keras.\n",
        "You can use PyTorch's nn.Module class to create custom layers and then compose them together using Python functions to create your network.\n",
        "Here's how you can implement a VGG-style CNN using the PyTorch functional API:"
      ],
      "metadata": {
        "id": "aCqztG05ctMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGGBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_convs):\n",
        "        super(VGGBlock, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(num_convs):\n",
        "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            in_channels = out_channels\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(VGG, self).__init__()\n",
        "        self.block1 = VGGBlock(3, 64, 2)\n",
        "        self.block2 = VGGBlock(64, 128, 2)\n",
        "        self.block3 = VGGBlock(128, 256, 3)\n",
        "        self.block4 = VGGBlock(256, 512, 3)\n",
        "        self.block5 = VGGBlock(512, 512, 3)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the VGG model\n",
        "model = VGG(num_classes=1000)\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "IKLWmtonc4cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we define a VGGBlock class that represents a single block in the VGG architecture. Then, we use this block to build the entire VGG network in the VGG class.\n",
        "You can adjust the number of output classes by passing the num_classes parameter when creating an instance of the model.\n",
        "This implementation follows a functional and modular approach similar to the Keras Functional API."
      ],
      "metadata": {
        "id": "CYsvVfp6c-6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JAX\n",
        "\n",
        "(from https://github.com/google/jax)\n",
        "\n",
        "JAX is Autograd and XLA, brought together for high-performance machine learning research.\n",
        "\n",
        "With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) via grad as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.\n",
        "\n",
        "What’s new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, jit. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python. You can even program multiple GPUs or TPU cores at once using pmap, and differentiate through the whole thing.\n",
        "\n",
        "Dig a little deeper, and you'll see that JAX is really an extensible system for composable function transformations. Both grad and jit are instances of such transformations. Others are vmap for automatic vectorization and pmap for single-program multiple-data (SPMD) parallel programming of multiple accelerators, with more to come.\n",
        "\n",
        "This is a research project, not an official Google product. Expect bugs and sharp edges. Please help by trying it out, reporting bugs, and letting us know what you think!\n",
        "\n"
      ],
      "metadata": {
        "id": "kKto42uxdIeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG in JAX\n",
        "\n",
        "In JAX, you can build a VGG-style CNN using the JAX neural network library, flax.\n",
        "Here's an example of how you can implement such a network:"
      ],
      "metadata": {
        "id": "ACm7Qm7y80Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "\n",
        "class VGGBlock(nn.Module):\n",
        "    out_channels: int\n",
        "    num_convs: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for _ in range(self.num_convs):\n",
        "            x = nn.Conv(self.out_channels, kernel_size=(3, 3), padding='SAME')(x)\n",
        "            x = nn.relu(x)\n",
        "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        return x\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    num_classes: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = VGGBlock(64, 2)(x)\n",
        "        x = VGGBlock(128, 2)(x)\n",
        "        x = VGGBlock(256, 3)(x)\n",
        "        x = VGGBlock(512, 3)(x)\n",
        "        x = VGGBlock(512, 3)(x)\n",
        "\n",
        "        x = x.mean(axis=(1, 2))  # Global Average Pooling\n",
        "        x = nn.Dense(self.num_classes)(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the VGG model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "input_shape = (1, 224, 224, 3)  # Batch size of 1, input shape\n",
        "model = VGG(num_classes=1000)\n",
        "params = model.init(rng, jnp.ones(input_shape, dtype=jnp.float32))\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J28ZEvUudg1o",
        "outputId": "cfc69fd8-9b8e-4edf-d844-49875203745b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "    # attributes\n",
            "    num_classes = 1000\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we use the linen module from Flax to define the VGGBlock and VGG classes.\n",
        "The @nn.compact decorator is used to define the forward pass of each block and the entire network.\n",
        "We create an instance of the VGG model and initialize its parameters using JAX's PRNGKey.\n",
        "Finally, we print a summary of the model architecture.\n",
        "\n",
        "You can adjust the num_classes parameter when creating an instance of the model to specify the number of output classes you need."
      ],
      "metadata": {
        "id": "Kt-My_vydwU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training models\n",
        "\n",
        "In these next examples we will se how we can train our models to perform multiclass classification on the cifar-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) with models defined in each of the frameworks presented above.\n",
        "\n",
        "##Note:\n",
        "These are only vary basic examples. When it becomes time to train your own models you should definitely look up more information. E.g how to efficiently tune hyperparameters, loading and transforming data and try to define more complex models. As well as how to save, load and use your model to perform inference on new data."
      ],
      "metadata": {
        "id": "JORJmTR5d4L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keras generators\n",
        "\n",
        "Here's an example of how you can create a training loop using data generators and the model.fit function\n",
        "in Keras for a Sequential model using the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "UrES0whMearM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.01,momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Create data generators for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the model using data generators\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
        "    steps_per_epoch=len(train_images) // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8Fmb5slwen8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this code:\n",
        "\n",
        "    We load the CIFAR-10 dataset and preprocess the data.\n",
        "    We create a Sequential model with convolutional and fully connected layers.\n",
        "    We compile the model with the SGD optimizer and categorical cross-entropy loss.\n",
        "    We create an ImageDataGenerator for data augmentation.\n",
        "    We use model.fit to train the model using the data generator.\n",
        "    Finally, we evaluate the model on the test data and print the test accuracy.\n",
        "\n",
        "You can adjust the batch size, number of epochs, and other hyperparameters as needed for your training."
      ],
      "metadata": {
        "id": "w9iU87gShsCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Torch dataloader\n",
        "\n",
        "In PyTorch, you can create a similar training loop using data loaders and manual iterations over the dataset.\n",
        "Here's how you can train a PyTorch model on the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "1EdD5APplUuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit.shortcuts import progress_bar\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the model similar to the keras sequential model above\n",
        "class net(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(net, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64*8*8, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = net(num_classes=10)\n",
        "\n",
        "# Define data transformations and create data loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=120),\n",
        "    transforms.RandomAffine(degrees=0,translate=[0,0.2],fill=0),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm(enumerate(trainloader, 0)):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Training Finished\")\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "H3H5k-NOlh6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "    We define a model, set up data transformations, and create data loaders for both the training and test datasets.\n",
        "\n",
        "    We define the loss function (CrossEntropyLoss) and optimizer (SGD).\n",
        "\n",
        "    The training loop iterates over the dataset, computes gradients, and updates the model's weights using backpropagation.\n",
        "\n",
        "    After training, we evaluate the model on the test dataset to calculate accuracy.\n",
        "\n",
        "Make sure you have the required libraries installed and a compatible GPU if you want to run this code on a GPU."
      ],
      "metadata": {
        "id": "wIfXQaisol3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training JAX model using data generators\n",
        "\n",
        "In JAX, you can create a training loop for a model using manual iteration and gradient updates. Here's how you can train a JAX model on the CIFAR-10 dataset:"
      ],
      "metadata": {
        "id": "QVrz8Sn-pPom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "from jax import jit, random, value_and_grad, vmap, pmap\n",
        "from jax.example_libraries import optimizers\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define the model (similar to the previous Keras and Pytorch examples)\n",
        "\n",
        "\n",
        "class net(nn.Module):\n",
        "    num_classes: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Conv(32, kernel_size=(3, 3), padding='SAME')(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = nn.Conv(64, kernel_size=(3, 3), padding='SAME')(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = nn.Conv(64, kernel_size=(3, 3), padding='SAME')(x)\n",
        "        x = nn.relu(x)\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        x = nn.Dense(features=10)(x)\n",
        "        x = nn.log_softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "input_shape = (64, 32, 32, 3)  # Batch size of 64, input shape\n",
        "model = net(num_classes=10)\n",
        "params = model.init(rng, jnp.ones(input_shape, dtype=jnp.float32))\n",
        "\n",
        "\n",
        "#load and preprocess data\n",
        "(x_train, y_train), (x_valid, y_valid) = cifar10.load_data()\n",
        "print(f\"\\nNumber of training samples: {len(x_train)} with samples shape: {x_train.shape[1:]}\")\n",
        "print(f\"Number of validation samples: {len(x_valid)} with samples shape: {x_valid.shape[1:]}\")\n",
        "\n",
        "\n",
        "x_train_normalized = jnp.array(x_train / 255.)\n",
        "x_valid_normalized = jnp.array(x_valid / 255.)\n",
        "\n",
        "# One hot encoding applied to the labels. We have 10\n",
        "# classes in the dataset, hence the depth of OHE would be 10\n",
        "y_train_ohe = jnp.squeeze(jax.nn.one_hot(y_train, num_classes=10))\n",
        "y_valid_ohe = jnp.squeeze(jax.nn.one_hot(y_valid, num_classes=10))\n",
        "\n",
        "print(f\"Training images shape:   {x_train_normalized.shape}  Labels shape: {y_train_ohe.shape}\")\n",
        "print(f\"Validation images shape: {x_valid_normalized.shape}  Labels shape: {y_valid_ohe.shape}\")\n",
        "\n",
        "#define functions for dataaugmentation\n",
        "def rotate_90(img):\n",
        "    \"\"\"Rotates an image by 90 degress k times.\"\"\"\n",
        "    return jnp.rot90(img, k=1, axes=(0, 1))\n",
        "\n",
        "\n",
        "def identity(img):\n",
        "    \"\"\"Returns an image as it is.\"\"\"\n",
        "    return img\n",
        "\n",
        "\n",
        "def flip_left_right(img):\n",
        "    \"\"\"Flips an image left/right direction.\"\"\"\n",
        "    return jnp.fliplr(img)\n",
        "\n",
        "\n",
        "def flip_up_down(img):\n",
        "    \"\"\"Flips an image in up/down direction.\"\"\"\n",
        "    return jnp.flipud(img)\n",
        "\n",
        "\n",
        "def random_rotate(img, rotate):\n",
        "    \"\"\"Randomly rotate an image by 90 degrees.\n",
        "\n",
        "    Args:\n",
        "        img: Array representing the image\n",
        "        rotate: Boolean for rotating or not\n",
        "    Returns:\n",
        "        Rotated or an identity image\n",
        "    \"\"\"\n",
        "\n",
        "    return jax.lax.cond(rotate, rotate_90, identity, img)\n",
        "\n",
        "\n",
        "def random_horizontal_flip(img, flip):\n",
        "    \"\"\"Randomly flip an image vertically.\n",
        "\n",
        "    Args:\n",
        "        img: Array representing the image\n",
        "        flip: Boolean for flipping or not\n",
        "    Returns:\n",
        "        Flipped or an identity image\n",
        "    \"\"\"\n",
        "\n",
        "    return jax.lax.cond(flip, flip_left_right, identity, img)\n",
        "\n",
        "\n",
        "def random_vertical_flip(img, flip):\n",
        "    \"\"\"Randomly flip an image vertically.\n",
        "\n",
        "    Args:\n",
        "        img: Array representing the image\n",
        "        flip: Boolean for flipping or not\n",
        "    Returns:\n",
        "        Flipped or an identity image\n",
        "    \"\"\"\n",
        "\n",
        "    return jax.lax.cond(flip, flip_up_down, identity, img)\n",
        "\n",
        "# All the above function are written to work on a single example.\n",
        "# We will use `vmap` to get a version of these functions that can\n",
        "# operate on a batch of images. We will also add the `jit` transformation\n",
        "# on top of it so that the whole pipeline can be compiled and executed faster\n",
        "random_rotate_jitted = jit(vmap(random_rotate, in_axes=(0, 0)))\n",
        "random_horizontal_flip_jitted = jit(vmap(random_horizontal_flip, in_axes=(0, 0)))\n",
        "random_vertical_flip_jitted = jit(vmap(random_vertical_flip, in_axes=(0, 0)))\n",
        "\n",
        "\n",
        "def augment_images(images, key):\n",
        "    \"\"\"Augment a batch of input images.\n",
        "\n",
        "    Args:\n",
        "        images: Batch of input images as a jax array\n",
        "        key: Seed/Key for random functions for generating booleans\n",
        "    Returns:\n",
        "        Augmented images with the same shape as the input images\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = len(images)\n",
        "\n",
        "    # 1. Rotation\n",
        "    key, subkey = random.split(key)\n",
        "    rotate = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n",
        "    augmented = random_rotate_jitted(images, rotate)\n",
        "\n",
        "    # 2. Flip horizontally\n",
        "    key, subkey = random.split(key)\n",
        "    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n",
        "    augmented = random_horizontal_flip_jitted(augmented, flip)\n",
        "\n",
        "    # 3. Flip vertically\n",
        "    key, subkey = random.split(key)\n",
        "    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n",
        "    augmented = random_vertical_flip_jitted(augmented, flip)\n",
        "\n",
        "    return augmented\n",
        "\n",
        "\n",
        "#define datagenerator\n",
        "def data_generator(images, labels, batch_size=64, is_valid=False, key=None):\n",
        "    \"\"\"Generates batches of data from a given dataset.\n",
        "\n",
        "    Args:\n",
        "        images: Image data represented by a ndarray\n",
        "        labels: One-hot enocded labels\n",
        "        batch_size: Number of data points in a single batch\n",
        "        is_valid: (Boolean) If validation data, then don't shuffle and\n",
        "                    don't apply any augmentation\n",
        "        key: PRNG key needed for augmentation\n",
        "    Yields:\n",
        "        Batches of images-labels pairs\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Calculate the total number of batches\n",
        "    num_batches = int(np.ceil(len(images) / batch_size))\n",
        "\n",
        "    # 2. Get the indices and shuffle them\n",
        "    indices = np.arange(len(images))\n",
        "\n",
        "    if not is_valid:\n",
        "        if key is None:\n",
        "             raise ValueError(\"A PRNG key is required if `aug` is set to True\")\n",
        "        else:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        curr_idx = indices[batch * batch_size: (batch+1) * batch_size]\n",
        "        batch_images = images[curr_idx]\n",
        "        batch_labels = labels[curr_idx]\n",
        "\n",
        "        if not is_valid:\n",
        "            batch_images = augment_images(batch_images, key=key)\n",
        "        yield batch_images, batch_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sanity Check: To make sure that the batches generated by the data\n",
        "# generator are of correct size, we will just pull a batch of data and\n",
        "# will check the shape of the images and the labels\n",
        "\n",
        "sample_data_gen = data_generator(\n",
        "    images=x_train_normalized,\n",
        "    labels=y_train_ohe,\n",
        "    batch_size=8,\n",
        "    is_valid=False,\n",
        "    key=random.PRNGKey(0)\n",
        ")\n",
        "\n",
        "sample_batch_images, sample_batch_labels = next(sample_data_gen)\n",
        "print(\"Batch of images is of shape: \", sample_batch_images.shape)\n",
        "print(\"Batch of labels is of shape: \", sample_batch_labels.shape)\n",
        "\n",
        "# Clean up unnecessary objects\n",
        "del sample_data_gen, sample_batch_images, sample_batch_labels\n",
        "\n",
        "\n",
        "def calculate_accuracy(params, batch_data):\n",
        "    \"\"\"Implements accuracy metric.\n",
        "\n",
        "    Args:\n",
        "        params: Parameters of the network\n",
        "        batch_data: A batch of data (images and labels)\n",
        "    Returns:\n",
        "        Accuracy for the current batch\n",
        "    \"\"\"\n",
        "    inputs, targets = batch_data\n",
        "    target_class = jnp.argmax(targets, axis=1)\n",
        "    predicted_class = jnp.argmax(model.apply(params,inputs), axis=1)\n",
        "    return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "\n",
        "# We will jit the train and test steps to make them more efficient\n",
        "@jit\n",
        "def train_step(step, opt_state, batch_data):\n",
        "    \"\"\"Implements train step.\n",
        "\n",
        "    Args:\n",
        "        step: Integer representing the step index\n",
        "        opt_state: Current state of the optimizer\n",
        "        batch_data: A batch of data (images and labels)\n",
        "    Returns:\n",
        "        Batch loss, batch accuracy, updated optimizer state\n",
        "    \"\"\"\n",
        "    params = get_params(opt_state)\n",
        "    batch_loss, batch_gradients = value_and_grad(loss_fn)(params, batch_data)\n",
        "    batch_accuracy = calculate_accuracy(params, batch_data)\n",
        "    return batch_loss, batch_accuracy, opt_update(step, batch_gradients, opt_state)\n",
        "\n",
        "\n",
        "@jit\n",
        "def test_step(opt_state, batch_data):\n",
        "    \"\"\"Implements train step.\n",
        "\n",
        "    Args:\n",
        "        opt_state: Current state of the optimizer\n",
        "        batch_data: A batch of data (images and labels)\n",
        "    Returns:\n",
        "        Batch loss, batch accuracy\n",
        "    \"\"\"\n",
        "    params = get_params(opt_state)\n",
        "    batch_loss = loss_fn(params, batch_data)\n",
        "    batch_accuracy = calculate_accuracy(params, batch_data)\n",
        "    return batch_loss, batch_accuracy\n",
        "\n",
        "\n",
        "# Define loss function\n",
        "\n",
        "def loss_fn(params, batch_data):\n",
        "    inputs, targets = batch_data\n",
        "    logits = model.apply(params, inputs)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=targets))\n",
        "    return loss\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Get the optimizer objects\n",
        "opt_init, opt_update, get_params = optimizers.momentum(step_size=LEARNING_RATE,mass=0.9)\n",
        "\n",
        "# Initialize the state of the optimizer using the parameters\n",
        "opt_state = opt_init(params)\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Initial rng key for the data generator\n",
        "key = random.PRNGKey(0)\n",
        "\n",
        "# Lists to record loss and accuracy for each epoch\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_accuracy = []\n",
        "validation_accuracy = []\n",
        "\n",
        "# Training\n",
        "for i in range(EPOCHS):\n",
        "    num_train_batches = len(x_train) // BATCH_SIZE\n",
        "    num_valid_batches = len(x_valid) // BATCH_SIZE\n",
        "\n",
        "    # Lists to store loss and accuracy for each batch\n",
        "    train_batch_loss, train_batch_acc = [], []\n",
        "    valid_batch_loss, valid_batch_acc = [], []\n",
        "\n",
        "    # Key to be passed to the data generator for augmenting\n",
        "    # training dataset\n",
        "    key, subkey = random.split(key)\n",
        "\n",
        "    # Initialize data generators\n",
        "    train_data_gen = data_generator(x_train_normalized,\n",
        "                                y_train_ohe,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                is_valid=False,\n",
        "                                key=key\n",
        "                               )\n",
        "\n",
        "    valid_data_gen = data_generator(x_valid_normalized,\n",
        "                               y_valid_ohe,\n",
        "                               batch_size=BATCH_SIZE,\n",
        "                               is_valid=True\n",
        "                               )\n",
        "\n",
        "    print(f\"Epoch: {i+1:<3}\", end=\" \")\n",
        "\n",
        "    # Training\n",
        "    for step in tqdm(range(num_train_batches)):\n",
        "        batch_data = next(train_data_gen)\n",
        "        loss_value, acc, opt_state = train_step(step, opt_state, batch_data)\n",
        "        train_batch_loss.append(loss_value)\n",
        "        train_batch_acc.append(acc)\n",
        "\n",
        "    # Evaluation on validation data\n",
        "    for step in tqdm(range(num_valid_batches)):\n",
        "        batch_data = next(valid_data_gen)\n",
        "        loss_value, acc = test_step(opt_state, batch_data)\n",
        "        valid_batch_loss.append(loss_value)\n",
        "        valid_batch_acc.append(acc)\n",
        "\n",
        "    # Loss for the current epoch\n",
        "    epoch_train_loss = np.mean(train_batch_loss)\n",
        "    epoch_valid_loss = np.mean(valid_batch_loss)\n",
        "\n",
        "    # Accuracy for the current epoch\n",
        "    epoch_train_acc = np.mean(train_batch_acc)\n",
        "    epoch_valid_acc = np.mean(valid_batch_acc)\n",
        "\n",
        "    training_loss.append(epoch_train_loss)\n",
        "    training_accuracy.append(epoch_train_acc)\n",
        "    validation_loss.append(epoch_valid_loss)\n",
        "    validation_accuracy.append(epoch_valid_acc)\n",
        "\n",
        "    print(f\"loss: {epoch_train_loss:.3f}   acc: {epoch_train_acc:.3f}  valid_loss: {epoch_valid_loss:.3f}  valid_acc: {epoch_valid_acc:.3f}\")"
      ],
      "metadata": {
        "id": "TzV1xZx9pxlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrained models\n",
        "\n",
        "Using pretrained models can be a great help in training deep learning models and convolutional neural networks. Especially in cases where data is limited relative to huge datasets such as ImageNet. The use of pretrained models can take different forms. A model can be used either as a feature extractor, i.e you take your data an pass it trough the model once to extract features and then train a model on those feature, or you finetune some, or all of the weights of the pretrained model on the new data."
      ],
      "metadata": {
        "id": "N6kH-sVNwJX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrained models in Keras\n",
        "\n",
        "To import a pre-trained VGG model with ImageNet weights in Keras,\n",
        "you can use the tf.keras.applications module, which provides pre-trained models with pre-trained weights.\n",
        "Here's how you can import a VGG model with ImageNet weights:"
      ],
      "metadata": {
        "id": "srcYn7g9wVGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, ReLU, Dropout\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights\n",
        "vgg_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Use ImageNet pre-trained weights\n",
        "    include_top=False,     # Exclude the fully connected layers (FC classification layer) when used on new data.\n",
        "    input_shape=(224, 224, 3)  # Specify the input shape of your data\n",
        ")\n",
        "\n",
        "input = Input(shape=(224,224,3))\n",
        "x = vgg_model(input)\n",
        "x = Flatten()(x)\n",
        "x = Dense(4096)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_classes,activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "kzrki9Pawivp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#And in Pytorch\n",
        "And here is how it is done in PyTorch using the torchvision library"
      ],
      "metadata": {
        "id": "yhUBkNzenA4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights, excluding the top layer\n",
        "vgg_model = models.vgg16(pretrained=True)\n",
        "vgg_model = nn.Sequential(*list(vgg_model.children())[:-1])  # Remove the top fully connected layer\n",
        "\n",
        "# Define a new fully connected layer with the number of classes in your new dataset\n",
        "num_classes = 10  # Replace with the number of classes in your new dataset\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(512, 4096),  # Example: Add a new fully connected layer\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, num_classes)  # Output layer with the new number of classes\n",
        ")\n",
        "\n",
        "# Combine the pre-trained VGG model and the new classifier\n",
        "model = nn.Sequential(\n",
        "    vgg_model,\n",
        "    nn.Flatten(),\n",
        "    classifier\n",
        ")\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "neQdbNc3nFqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f454b376-e66d-4020-d64d-92e810676747"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU(inplace=True)\n",
            "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): ReLU(inplace=True)\n",
            "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): ReLU(inplace=True)\n",
            "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (13): ReLU(inplace=True)\n",
            "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): ReLU(inplace=True)\n",
            "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (18): ReLU(inplace=True)\n",
            "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (20): ReLU(inplace=True)\n",
            "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (22): ReLU(inplace=True)\n",
            "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (25): ReLU(inplace=True)\n",
            "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (27): ReLU(inplace=True)\n",
            "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (29): ReLU(inplace=True)\n",
            "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  )\n",
            "  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (2): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Note on input shapes\n",
        "\n",
        "Be aware that the model obtained from keras.applications and torchvision have been trained on images of a specific shape. The VGG network shown in this example for instance is trained on ImageNet image with 224x224 image crops. When removing the original FC classification layer (and other layers that expect specific shapes), models are in principle agnostic to the input shape. But, because of the downsampling in the spatial dimensions that happens in the network, there is a lower bound on the input shape.\n",
        "\n",
        "(There is also an upper bound, but this is generally determined by the amount of memory availiable)\n",
        "\n",
        "So for instance, we can load the VGG network with diffenrent input shapes. But if the initial shape is to small, the operations on feature maps inside the network will fail at some point."
      ],
      "metadata": {
        "id": "8lHwnF_mrTgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights\n",
        "vgg_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Use ImageNet pre-trained weights\n",
        "    include_top=False,     # Exclude the fully connected layers (FC classification layer) when used on new data.\n",
        "    input_shape=(128, 128, 3)  # Specify the input shape of your data\n",
        ")\n",
        "\n",
        "# Print a summary of the model architecture in this case the spatial shape after the final pooling layer is 4x4\n",
        "vgg_model.summary()"
      ],
      "metadata": {
        "id": "iCfUiMSDtV-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we try with an even smaller input size, the call to the application function will fail.\n"
      ],
      "metadata": {
        "id": "RvwyRal7kIPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights\n",
        "vgg_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Use ImageNet pre-trained weights\n",
        "    include_top=False,     # Exclude the fully connected layers (FC classification layer) when used on new data.\n",
        "    input_shape=(28, 28, 3)  # Specify the input shape of your data\n",
        ")\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "vgg_model.summary()"
      ],
      "metadata": {
        "id": "wOJDRv8Mt2zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above case, there are basically two options to fix the issue:\n",
        "\n",
        "1. Resize the input to match the required shape\n",
        "2. Create a model which is a shallower version of the orginal model with identical layers and layer names (but new shapes) and load the weights from the pretrained model into the new model. (No example will be shown here, but for keras you can look at the .get_weights() and .set_weights() methods)\n",
        "\n",
        "Luckily, based on the error message above, we see that the minimum required input shape is 32x32x3, which matches the dimensions of the cifar 10 dataset. Meaning that we can use the VGG model for finetuning on this dataset.\n",
        "\n",
        "Example below.\n",
        "\n",
        "As you will see, now that we are beginning to work with vary large models training time increases significantly. Especially when training on CPU. If you go to: \"Edit --> Notebook settings \" you can select a hardware accelarator for the notebook. Select GPU if availiable. Try to run the code below both with, and without GPU acceleration to appreciate the difference.\n",
        "\n",
        "(don't finish the training using CPU. It is going to take forever...)"
      ],
      "metadata": {
        "id": "A7wPyWIjuZU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, ReLU, Dropout\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights\n",
        "vgg_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Use ImageNet pre-trained weights\n",
        "    include_top=False,     # Exclude the fully connected layers (FC classification layer) when used on new data.\n",
        "    input_shape=(32, 32, 3)  # Specify the input shape of your data\n",
        ")\n",
        "\n",
        "input = Input(shape=(32,32,3))\n",
        "x = vgg_model(input)\n",
        "x = Flatten()(x)\n",
        "x = Dense(4096)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_classes,activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = out)\n",
        "\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.0001,momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#we train the model with a smaller learning rate than in the previous example. This is generally considered good practice as to not change the pretrained weights to much in the update stage\n",
        "\n",
        "# Create data generators for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the model using data generators\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
        "    steps_per_epoch=len(train_images) // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "v2SiWluLvqpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\"Subsampling\" pretrained networks\n",
        "\n",
        "when using pretrained networks it can be tempting to just take the biggest and best model trained on ImageNet and apply it to your own data. But remember, with increasing depth of the networks, the filters that are learned are more and more specialized to the data they where originally trained on. Thus, if you are working with data that are very dissimilar to ImageNet, it may be a good idea to only reuse some of the weights from a pretrained model, as the filters in the shallow parts of a pretrained model will be of more general nature."
      ],
      "metadata": {
        "id": "GYtkXdsKoW9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, ReLU, Dropout, GlobalAveragePooling2D\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "# Create data generators for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# Import the VGG16 model with ImageNet weights\n",
        "vgg_weights = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Use ImageNet pre-trained weights\n",
        "    include_top=False,     # Exclude the fully connected layers (FC classification layer) when used on new data.\n",
        "    input_shape=(32, 32, 3)  # Specify the input shape of your data\n",
        ")\n",
        "\n",
        "vgg_weights.summary()\n",
        "\n",
        "vgg_sub_in = vgg_weights.input\n",
        "\n",
        "vgg_sub_out = vgg_weights.layers[10].output\n",
        "\n",
        "vgg_sub = Model(inputs = vgg_sub_in, outputs = vgg_sub_out)\n",
        "\n",
        "\n",
        "vgg_sub.summary()\n",
        "\n",
        "\n",
        "input = Input(shape=(32,32,3))\n",
        "x = vgg_sub(input)\n",
        "x = GlobalAveragePooling2D()(x) #we change the flatten layer used in the model above to an average pooling layer to save parameters\n",
        "x = Dense(4096)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "out = Dense(num_classes,activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = out)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.0001,momentum=0.9),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model using data generators\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
        "    steps_per_epoch=len(train_images) // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tA4avXmBoepE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read much more about Keras and Pytorch online and both frameworks have a lot of good toturials on their respective websites:\n",
        "\n",
        "- Keras: https://keras.io/\n",
        "- Pytorch: https://pytorch.org/tutorials/\n"
      ],
      "metadata": {
        "id": "gORmGSllAAMz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wt9ETqBAPTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}